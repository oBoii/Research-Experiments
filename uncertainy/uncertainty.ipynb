{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-05-22T09:10:55.324698900Z",
     "start_time": "2024-05-22T09:10:55.174669100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import argparse\n",
    "import sys\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## Torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "import shutil\n",
    "from scipy.ndimage import shift\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2872261e06ec0e70",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:10:55.334240600Z",
     "start_time": "2024-05-22T09:10:55.194459500Z"
    }
   },
   "outputs": [],
   "source": [
    "h = {  # hyperparameters\n",
    "    'dataset': 'RADIO',  #'MNIST_SHIFT',  #'MNIST',\n",
    "    'in_channels': 2,\n",
    "    'hidden_channels': 32,\n",
    "    'epochs': 2,\n",
    "    'learning_rate': 1e-4,\n",
    "    'checkpoint_path': './saved_models',\n",
    "    'dataset_path': '../data/',\n",
    "    'batch_size': 128,\n",
    "    'num_workers': 8,\n",
    "    'persistent_workers': True,\n",
    "    'pin_memory': True,\n",
    "    'train': False,  # Set this to false if you only want to evaluate the model\n",
    "    'use_wandb': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Check if the script is being run in a Jupyter notebook\n",
    "if 'ipykernel' not in sys.modules:\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    for key, value in h.items():\n",
    "        if isinstance(value, bool):\n",
    "            parser.add_argument(f'--{key}', type=bool, default=value)\n",
    "        elif isinstance(value, int):\n",
    "            parser.add_argument(f'--{key}', type=int, default=value)\n",
    "        elif isinstance(value, float):\n",
    "            parser.add_argument(f'--{key}', type=float, default=value)\n",
    "        else:  # for str and potentially other types\n",
    "            parser.add_argument(f'--{key}', type=type(value), default=value)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Overwrite the default hyperparameters with the command-line arguments\n",
    "    h.update(vars(args))\n",
    "\n",
    "# In terminal, run:\n",
    "# python main.py --model SinkhornConvNetV2 --epochs 30"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:10:55.339276900Z",
     "start_time": "2024-05-22T09:10:55.209231600Z"
    }
   },
   "id": "782efec0071d7437"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# if h['model'] == 'Sink':\n",
    "#     SinkhornConvNet = SinkhornConvNetV1\n",
    "# elif h['model'] == 'SinkV2':\n",
    "#     SinkhornConvNet = SinkhornConvNetV2\n",
    "# elif h['model'] == 'SinkV3':\n",
    "#     SinkhornConvNet = SinkhornConvNetV3\n",
    "# else:\n",
    "#     raise ValueError(f\"Unknown model: {h['model']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:10:55.346825500Z",
     "start_time": "2024-05-22T09:10:55.329213Z"
    }
   },
   "id": "b515de4dbeeb21c9"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "140f1eaf4a294ae7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:10:55.414299200Z",
     "start_time": "2024-05-22T09:10:55.334240600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1568])\n",
      "torch.Size([64, 6272])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([64, 4, 1, 14, 14]), torch.Size([64, 4, 4]))"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # Example usage\n",
    "# model = SinkhornConvNet(\n",
    "#     in_channels=1, num_pieces=2, image_size=28, hidden_channels=32, kernel_size=5, tau=0.1, n_sink_iter=20)\n",
    "# \n",
    "# random_pieces = torch.rand((64, 4, 1, 14, 14))\n",
    "# res1, res2 = model(random_pieces)\n",
    "# res1.shape, res2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2361bc0ae34c860c"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39e7deae86e70d1d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:10:56.814201800Z",
     "start_time": "2024-05-22T09:10:55.379029Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "if h['dataset'] == 'MNIST':\n",
    "    trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "    testset = datasets.MNIST(root='../data', train=False, transform=transform)\n",
    "elif h['dataset'] == 'MNIST_SHIFT':\n",
    "    trainset = MNIST_SHIFT(root='../data', train=True, download=True, transform=transform)\n",
    "    testset = MNIST_SHIFT(root='../data', train=False, transform=transform)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {h['dataset']}\")\n",
    "\n",
    "train_loader = DataLoader(trainset, h['batch_size'], drop_last=True, shuffle=True,\n",
    "                          num_workers=h['num_workers'], persistent_workers=h['persistent_workers'],\n",
    "                          pin_memory=h['pin_memory'])\n",
    "test_loader = DataLoader(testset, h['batch_size'], drop_last=False, shuffle=False,\n",
    "                         num_workers=h['num_workers'], persistent_workers=h['persistent_workers'],\n",
    "                         pin_memory=h['pin_memory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "72ca7ac9073ef3ea",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:11:00.804195600Z",
     "start_time": "2024-05-22T09:11:00.784408100Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCallbacks(pl.Callback):\n",
    "    def __init__(self, plot_every_n_epoch, num_pieces, wandb_logger: WandbLogger):\n",
    "        super().__init__()\n",
    "        self.plot_every_n_epoch = plot_every_n_epoch\n",
    "        self.num_pieces = num_pieces\n",
    "        self.wandb_logger = wandb_logger\n",
    "\n",
    "    def assemble_image(self, pieces):\n",
    "        # pieces: [num_pieces, channels, height // num_pieces, width // num_pieces]\n",
    "        num_pieces, channels, piece_height, piece_width = pieces.shape\n",
    "        num_pieces_side = int(num_pieces ** 0.5)\n",
    "\n",
    "        # Reshape to [num_pieces_side, num_pieces_side, channels, piece_height, piece_width]\n",
    "        pieces = pieces.view(num_pieces_side, num_pieces_side, channels, piece_height, piece_width)\n",
    "\n",
    "        # Permute to [channels, num_pieces_side, piece_height, num_pieces_side, piece_width]\n",
    "        pieces = pieces.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # Reshape to [channels, height, width]\n",
    "        image = pieces.contiguous().view(channels, num_pieces_side * piece_height, num_pieces_side * piece_width)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def log_images(self, trainer, pl_module, loader, prefix):\n",
    "        if (trainer.current_epoch % self.plot_every_n_epoch == 0) or (trainer.current_epoch == h['epochs'] - 1):\n",
    "            pl_module.eval()\n",
    "            image_batch, label_batch = next(iter(loader))\n",
    "            pieces, random_pieces, _ = batch_chunk_image(image_batch, self.num_pieces)\n",
    "            pieces, random_pieces = pieces.to(pl_module.device), random_pieces.to(pl_module.device)\n",
    "\n",
    "            ordered_pieces, permutation_matrices = pl_module(random_pieces)\n",
    "            # Assemble the pieces into a single image before logging\n",
    "            nb_ims = min(ordered_pieces.shape[0], 10)\n",
    "            for i in range(nb_ims):\n",
    "                initial_image = self.assemble_image(random_pieces[i])\n",
    "                ordered_image = self.assemble_image(ordered_pieces[i])\n",
    "                ground_truth_image = self.assemble_image(pieces[i])\n",
    "\n",
    "                # log to wandb\n",
    "                self.wandb_logger.experiment.log(\n",
    "                    {f\"{prefix}_predicted_image/img_{i}\": wandb.Image(ordered_image.cpu().squeeze(),\n",
    "                                                                      caption=f\"Label {label_batch[i]}\"),\n",
    "                     f\"{prefix}_ground_truth/img_{i}\": wandb.Image(ground_truth_image.cpu().squeeze(),\n",
    "                                                                   caption=f\"Label {label_batch[i]}\"),\n",
    "                     f\"{prefix}_input/img_{i}\": wandb.Image(initial_image.cpu().squeeze(),\n",
    "                                                            caption=f\"Label {label_batch[i]}\")},\n",
    "                    step=trainer.global_step)\n",
    "\n",
    "            pl_module.train()\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: SinkhornConvNet):\n",
    "        self.log_images(trainer, pl_module, train_loader, \"TRAIN\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: SinkhornConvNet):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        self.log_images(trainer, pl_module, test_loader, \"TEST_SET_train_end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9614d75c960f9ff8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:11:00.816801200Z",
     "start_time": "2024-05-22T09:11:00.804195600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SinkhornConvNet(in_channels=h['in_channels'],\n",
    "                        num_pieces=h['num_pieces'],\n",
    "                        image_size=h['image_size'],\n",
    "                        hidden_channels=h['hidden_channels'],\n",
    "                        kernel_size=h['kernel_size'],\n",
    "                        tau=h['tau'],\n",
    "                        n_sink_iter=h['n_sink_iter'])\n",
    "\n",
    "if h['train']:\n",
    "    if h['use_wandb']:\n",
    "        date_identifier = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        wandb_logger = WandbLogger()\n",
    "        wandb.init(project='sinkhorn_netw', config=h, entity='oboii',\n",
    "                   name=f'{h[\"model\"]}_{h[\"dataset\"]}_{h[\"num_pieces\"]}_pieces_{date_identifier}')\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=h['epochs'],\n",
    "        callbacks=[CustomCallbacks(plot_every_n_epoch=1, num_pieces=h['num_pieces'],\n",
    "                                   wandb_logger=wandb_logger)] if h['use_wandb'] else None,\n",
    "        logger=wandb_logger if h['use_wandb'] else None,\n",
    "        limit_train_batches=h['dataset_percent'],\n",
    "        limit_val_batches=h['dataset_percent'],\n",
    "        limit_test_batches=h['dataset_percent'])\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, val_dataloaders=test_loader)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(h['checkpoint_path'], 'model.pth'))\n",
    "\n",
    "    # Finish the run if we it was running\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "628a377030e96d13",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:11:04.384394500Z",
     "start_time": "2024-05-22T09:11:04.364224100Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
