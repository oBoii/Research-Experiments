{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-16T12:41:54.833715300Z",
     "start_time": "2024-05-16T12:41:54.817486100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## Torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "h = {  # hyperparameters\n",
    "    'in_channels': 1,\n",
    "    'num_pieces': 8,\n",
    "    'image_size': 28,\n",
    "    'hidden_channels': 32,\n",
    "    'kernel_size': 5,\n",
    "    'tau': 0.1,\n",
    "    'n_sink_iter': 20,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 1e-4,\n",
    "    'checkpoint_path': './saved_models',\n",
    "    'dataset_path': './data',\n",
    "    'batch_size': 64\n",
    "}\n",
    "train = True  # Set this to false if you only want to evaluate the model\n",
    "use_wandb = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T12:41:54.867371Z",
     "start_time": "2024-05-16T12:41:54.833715300Z"
    }
   },
   "id": "2872261e06ec0e70"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def log_sinkhorn(log_alpha, n_iter):\n",
    "    \"\"\"Performs incomplete Sinkhorn normalization to log_alpha.\n",
    "    By a theorem by Sinkhorn and Knopp [1], a sufficiently well-behaved  matrix\n",
    "    with positive entries can be turned into a doubly-stochastic matrix\n",
    "    (i.e. its rows and columns add up to one) via the successive row and column\n",
    "    normalization.\n",
    "\n",
    "    [1] Sinkhorn, Richard and Knopp, Paul.\n",
    "    Concerning nonnegative matrices and doubly stochastic\n",
    "    matrices. Pacific Journal of Mathematics, 1967\n",
    "    Args:\n",
    "      log_alpha: 2D tensor (a matrix of shape [N, N])\n",
    "        or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n",
    "      n_iters: number of sinkhorn iterations (in practice, as little as 20\n",
    "        iterations are needed to achieve decent convergence for N~100)\n",
    "    Returns:\n",
    "      A 3D tensor of close-to-doubly-stochastic matrices (2D tensors are\n",
    "        converted to 3D tensors with batch_size equals to 1)\n",
    "    \"\"\"\n",
    "    for _ in range(n_iter):\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
    "    return log_alpha.exp()\n",
    "\n",
    "\n",
    "def matching(alpha):\n",
    "    # Negate the probability matrix to serve as cost matrix. This function\n",
    "    # yields two lists, the row and colum indices for all entries in the\n",
    "    # permutation matrix we should set to 1.\n",
    "    row, col = linear_sum_assignment(-alpha)\n",
    "\n",
    "    # Create the permutation matrix.\n",
    "    permutation_matrix = coo_matrix((np.ones_like(row), (row, col))).toarray()\n",
    "    return torch.from_numpy(permutation_matrix)\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, device='cpu', eps=1e-20):\n",
    "    \"\"\"Samples arbitrary-shaped standard gumbel variables.\n",
    "    Args:\n",
    "      shape: list of integers\n",
    "      eps: float, for numerical stability\n",
    "    Returns:\n",
    "      A sample of standard Gumbel random variables\n",
    "    \"\"\"\n",
    "    u = torch.rand(shape, device=device)\n",
    "    return -torch.log(-torch.log(u + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_sinkhorn(log_alpha, tau, n_iter):\n",
    "    \"\"\" Sample a permutation matrix from the Gumbel-Sinkhorn distribution\n",
    "    with parameters given by log_alpha and temperature tau.\n",
    "\n",
    "    Args:\n",
    "      log_alpha: Logarithm of assignment probabilities. In our case this is\n",
    "        of dimensionality [num_pieces, num_pieces].\n",
    "      tau: Temperature parameter, the lower the value for tau the more closely\n",
    "        we follow a categorical sampling.\n",
    "    \"\"\"\n",
    "    # Sample Gumbel noise.\n",
    "    gumbel_noise = sample_gumbel(log_alpha.shape, device=log_alpha.device)\n",
    "\n",
    "    # Apply the Sinkhorn operator!\n",
    "    sampled_perm_mat = log_sinkhorn((log_alpha + gumbel_noise) / tau, n_iter)\n",
    "    return sampled_perm_mat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T12:41:54.867371Z",
     "start_time": "2024-05-16T12:41:54.847696900Z"
    }
   },
   "id": "caaa9de4da4da98d"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_image(image: torch.Tensor, num_pieces: int):\n",
    "    \"\"\"Randomly chunk a single image.\n",
    "    Args:\n",
    "      image: Image [channels, height, width].\n",
    "\n",
    "    Returns:\n",
    "      pieces: Image chunks in their original positions. [num_pieces, channels,\n",
    "        height // num_pieces, width // num_pieces]\n",
    "      random_pieces: Image chunks in their randomly permuted positions.\n",
    "      permute_index: List of permuted indices.\n",
    "    \"\"\"\n",
    "    # Get image dimensions.\n",
    "    height, width = image.shape[-2:]\n",
    "\n",
    "    # Get piece dimensions.\n",
    "    piece_height = height // num_pieces\n",
    "    piece_width = width // num_pieces\n",
    "    pieces = []\n",
    "\n",
    "    # Obtain indices for each of the image chunks.\n",
    "    for p_h in range(num_pieces):\n",
    "        for p_w in range(num_pieces):\n",
    "            left = p_w * piece_width\n",
    "            right = left + piece_width\n",
    "            top = p_h * piece_height\n",
    "            bottom = top + piece_height\n",
    "            piece = image[:, top:bottom, left:right]\n",
    "            pieces.append(piece)\n",
    "\n",
    "    pieces = torch.stack(pieces, 0)\n",
    "\n",
    "    # Randomly permute the index of the pieces.\n",
    "    permute_index = torch.randperm(num_pieces ** 2)\n",
    "    random_pieces = pieces[permute_index]\n",
    "    return pieces, random_pieces, permute_index\n",
    "\n",
    "\n",
    "def batch_chunk_image(images: torch.Tensor, num_pieces: int):\n",
    "    \"\"\"Randomly chunk a batch of images.\n",
    "    Args:\n",
    "      image: Images [batch, channels, height, width].\n",
    "\n",
    "    Returns:\n",
    "      pieces: Batch of image chunks in their original positions. [batch,\n",
    "        num_pieces, channels, height // num_pieces, width // num_pieces]\n",
    "      random_pieces: Batch of image chunks in their randomly permuted positions.\n",
    "         [batch, num_pieces, channels, height // num_pieces, width // num_pieces]\n",
    "      permute_index: Batch of permutation lists. [batch, num_pieces**2]\n",
    "    \"\"\"\n",
    "    batch_pieces, batch_random_pieces, batch_permute_index = [], [], []\n",
    "    for image in images:\n",
    "        pieces, random_pieces, permute_index = chunk_image(image, num_pieces)\n",
    "\n",
    "        batch_pieces.append(pieces)\n",
    "        batch_random_pieces.append(random_pieces)\n",
    "        batch_permute_index.append(permute_index)\n",
    "    return torch.stack(batch_pieces, 0), torch.stack(batch_random_pieces, 0), torch.stack(batch_permute_index, 0)\n",
    "\n",
    "\n",
    "def inverse_permutation_for_image(X, permutation_matrix):\n",
    "    # temp, make `permutation_matrix` transposed\n",
    "    # permutation_matrix = permutation_matrix.transpose(1, 2)\n",
    "    \n",
    "    \"\"\"Apply the inverse of a permutation (its transpose) to a batch of image\n",
    "       chunks.\n",
    "    Args:\n",
    "      X: Batched sets of image chunks. [batch, num_pieces, channels, height, width]\n",
    "      permutation_matrix: float, for numerical stability\n",
    "\n",
    "    Returns:\n",
    "      Permuted set of image chunks.\n",
    "    \"\"\"\n",
    "    return torch.einsum(\"bpq,bpchw->bqchw\", (permutation_matrix, X)).contiguous()\n",
    "\n",
    "\n",
    "# Example of `inverse_permutation_for_image`\n",
    "X = torch.rand((64, 4, 1, 14, 14))\n",
    "permutation_matrix = torch.rand((64, 4, 4))\n",
    "first1 = inverse_permutation_for_image(X, permutation_matrix)[0]  # (4, 1, 14, 14)\n",
    "first1 = first1.flatten()  # (1, 784)\n",
    "\n",
    "X_first = X[0]  # (4, 1, 14, 14)\n",
    "X_first = X_first.flatten(start_dim=1)  # (4, 196)\n",
    "\n",
    "permutation_matrix_first = permutation_matrix[0]  # (4, 4)\n",
    "permutation_matrix_first = permutation_matrix_first.T  # (4, 4)\n",
    "X_first_permuted = torch.mm(permutation_matrix_first, X_first)  # (4, 196)\n",
    "\n",
    "X_first_permuted = X_first_permuted.reshape(4, 1, 14, 14)  # (4, 1, 14, 14)\n",
    "X_first_permuted = X_first_permuted.flatten()  # (1, 784)\n",
    "\n",
    "# e.g.: P^T*X = \\tilde{X}, with row(X) = all pixels of a single piece\n",
    "torch.allclose(first1, X_first_permuted)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T12:41:54.879001Z",
     "start_time": "2024-05-16T12:41:54.867371Z"
    }
   },
   "id": "5bcb9f99ea136d5f"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([64, 4, 1, 14, 14]), torch.Size([64, 4, 4]))"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SinkhornConvNet(pl.LightningModule):\n",
    "    def __init__(self, in_channels, num_pieces, image_size, hidden_channels, kernel_size, tau=1.0, n_sink_iter=20):\n",
    "        super().__init__()\n",
    "\n",
    "        # store these for later use.\n",
    "        self.tau = tau\n",
    "        self.n_sink_iter = n_sink_iter\n",
    "        self.num_pieces = num_pieces\n",
    "\n",
    "        self.g_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_channels)\n",
    "        )\n",
    "\n",
    "        # calculate the size of a single piece in pixels\n",
    "        piece_size = image_size // num_pieces\n",
    "\n",
    "        # calculate the size of a single piece in pixels after 1 max pooling\n",
    "        piece_size_after_conv = (piece_size) // (2 * 1)\n",
    "\n",
    "        self.g_2 = nn.Linear(piece_size_after_conv ** 2 * hidden_channels, num_pieces ** 2, bias=False)\n",
    "\n",
    "    def forward(self, batch_pieces):\n",
    "        # in: (64, 4, 1, 14, 14): 4 pieces of 14x14 images\n",
    "        # out: (64, 4, 1, 14, 14), (64, 4, 4): 4 pieces of 14x14 images and permutation matrices\n",
    "        batch_size = batch_pieces.shape[0]\n",
    "\n",
    "        # Switch batch and piece dimensions. We want to apply the same network to each of the pieces.\n",
    "        pieces = batch_pieces.transpose(0, 1).contiguous()  # (4, 64, 1, 14, 14)\n",
    "\n",
    "        # Apply g_1 to each of the pieces.\n",
    "        conv_pieces = []\n",
    "        for piece in pieces:\n",
    "            piece = self.g_1(piece)\n",
    "            conv_piece = piece.reshape(batch_size, -1)\n",
    "            conv_pieces.append(conv_piece)\n",
    "\n",
    "        # Apply g_2 to each of the pieces.\n",
    "        latent_pieces = []\n",
    "        for piece in conv_pieces:\n",
    "            latent_piece = self.g_2(piece)\n",
    "            latent_pieces.append(latent_piece)\n",
    "\n",
    "        # Create a matrix of log unnormalized assignment probabilities. After this\n",
    "        # the batch dimension is batch in the first position.\n",
    "        log_alphas = torch.stack(latent_pieces, 1)  # (64, 4, 4)\n",
    "\n",
    "        # During training, we sample from the Gumbel-Sinkhorn distribution.\n",
    "        if self.training:\n",
    "            permutation_matrices = gumbel_sinkhorn(log_alphas, tau=self.tau, n_iter=self.n_sink_iter)\n",
    "\n",
    "        # During eval, we solve the linear assignment problem.\n",
    "        else:\n",
    "            permutation_matrices = torch.stack([\n",
    "                matching(log_alpha)\n",
    "                for log_alpha in log_alphas.cpu().detach().numpy()]\n",
    "            ).float().to(log_alphas.device)\n",
    "\n",
    "        # We obtain the ordered pieces as predicted by our network\n",
    "        ordered_pieces = inverse_permutation_for_image(batch_pieces, permutation_matrices)\n",
    "\n",
    "        # Return the ordered pieces, along with the predicted permutation.\n",
    "        # We will inspect the predicted permutation matrices during test time.\n",
    "        return ordered_pieces, permutation_matrices\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, _ = batch\n",
    "        pieces, random_pieces, _ = batch_chunk_image(inputs, self.num_pieces)\n",
    "        pieces, random_pieces = pieces.to(self.device), random_pieces.to(self.device)\n",
    "        ordered_pieces, _ = self(random_pieces)\n",
    "        loss = torch.nn.functional.mse_loss(ordered_pieces, pieces, reduction='sum')\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = SinkhornConvNet(\n",
    "    in_channels=1, num_pieces=2, image_size=28, hidden_channels=32, kernel_size=5, tau=0.1, n_sink_iter=20)\n",
    "\n",
    "random_pieces = torch.rand((64, 4, 1, 14, 14))\n",
    "res1, res2 = model(random_pieces)\n",
    "res1.shape, res2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T12:41:54.916216600Z",
     "start_time": "2024-05-16T12:41:54.887609Z"
    }
   },
   "id": "140f1eaf4a294ae7"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, h['batch_size'], drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, h['batch_size'], drop_last=False, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T12:41:54.997619100Z",
     "start_time": "2024-05-16T12:41:54.910993700Z"
    }
   },
   "id": "8984361914d18b55"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.17.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\GitHub\\Nomalizing Flow\\wandb\\run-20240516_144155-g21ge6g8</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw/runs/g21ge6g8' target=\"_blank\">sinkhorn_mnist_8_pieces_20240516-144154</a></strong> to <a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw' target=\"_blank\">https://wandb.ai/eccv_tanmoy/sinkhorn_netw</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw/runs/g21ge6g8' target=\"_blank\">https://wandb.ai/eccv_tanmoy/sinkhorn_netw/runs/g21ge6g8</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | g_1  | Sequential | 896   \n",
      "1 | g_2  | Linear     | 2.0 K \n",
      "------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff3ec7e8d6c34387a1731677de834151"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 59345.79\n",
      "Epoch 1, Loss 47794.86\n",
      "Epoch 2, Loss 48249.23\n",
      "Epoch 3, Loss 43408.73\n",
      "Epoch 4, Loss 47246.44\n",
      "Epoch 5, Loss 48970.45\n",
      "Epoch 6, Loss 49926.63\n",
      "Epoch 7, Loss 46394.09\n",
      "Epoch 8, Loss 49129.89\n",
      "Epoch 9, Loss 45116.65\n",
      "Epoch 10, Loss 47061.22\n",
      "Epoch 11, Loss 48443.78\n",
      "Epoch 12, Loss 44098.48\n",
      "Epoch 13, Loss 45729.03\n",
      "Epoch 14, Loss 47385.88\n",
      "Epoch 15, Loss 48803.26\n",
      "Epoch 16, Loss 46570.28\n",
      "Epoch 17, Loss 45688.52\n",
      "Epoch 18, Loss 45219.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss 46217.16\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "317fe527c61148dc9283e37ef0325d4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▅▄▃▄▃▃▂▃▃▃▂▂▃▄▂▃▃▂▂▁▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss</td><td>46526.86719</td></tr><tr><td>trainer/global_step</td><td>18699</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">sinkhorn_mnist_8_pieces_20240516-144154</strong> at: <a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw/runs/g21ge6g8' target=\"_blank\">https://wandb.ai/eccv_tanmoy/sinkhorn_netw/runs/g21ge6g8</a><br/> View project at: <a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw' target=\"_blank\">https://wandb.ai/eccv_tanmoy/sinkhorn_netw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240516_144155-g21ge6g8\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "date_identifier = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if use_wandb:\n",
    "    wandb_logger = WandbLogger()\n",
    "    wandb.init(project='sinkhorn_netw',\n",
    "               name=f'sinkhorn_mnist_{h[\"num_pieces\"]}_pieces_{date_identifier}')\n",
    "\n",
    "    wandb.config.update(h)\n",
    "\n",
    "# Initialize the model\n",
    "model = SinkhornConvNet(in_channels=h['in_channels'],\n",
    "                        num_pieces=h['num_pieces'],\n",
    "                        image_size=h['image_size'],\n",
    "                        hidden_channels=h['hidden_channels'],\n",
    "                        kernel_size=h['kernel_size'],\n",
    "                        tau=h['tau'],\n",
    "                        n_sink_iter=h['n_sink_iter'])\n",
    "\n",
    "if train:\n",
    "    # log_progress_callback; simply prints the progress to the console\n",
    "    class LogProgressCallback(pl.Callback):\n",
    "        def on_train_epoch_end(self, trainer, pl_module):\n",
    "            print(f\"Epoch {trainer.current_epoch}, \"\n",
    "                  f\"Loss {trainer.callback_metrics['train_loss']:.2f}\")\n",
    "\n",
    "\n",
    "    # Pass the callback to the Trainer\n",
    "\n",
    "    # Pass the callback to the Trainer\n",
    "    trainer = pl.Trainer(max_epochs=h['epochs'], callbacks=[LogProgressCallback()],\n",
    "                         logger=wandb_logger if use_wandb else None)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(h['checkpoint_path'], 'model.pth'))\n",
    "\n",
    "    # Finish the run if we it was running\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T13:18:18.150275900Z",
     "start_time": "2024-05-16T12:41:55.007675900Z"
    }
   },
   "id": "9614d75c960f9ff8"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 64 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 64 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./saved_models/model.pth\"))\n",
    "\n",
    "image_batch, _ = next(iter(test_loader))\n",
    "pieces, random_pieces, perm_list = batch_chunk_image(image_batch, h['num_pieces'])\n",
    "# pieces, random_pieces = pieces.to(device), random_pieces.to(device)\n",
    "\n",
    "# Make sure we are evaluating!\n",
    "model.eval()\n",
    "\n",
    "# Predict the correctly ordered pieces.\n",
    "predicted_pieces, _ = model(random_pieces)\n",
    "\n",
    "# Select an image from the batch.\n",
    "batch_idx = 3\n",
    "\n",
    "# Plot the original scrambed image.\n",
    "figs, axs = plt.subplots(h['num_pieces'], h['num_pieces'], figsize=(4, 4), sharex=True, sharey=True)\n",
    "# remove x, y ticks\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "for idx, piece in enumerate(random_pieces[batch_idx]):\n",
    "    axs[idx // h['num_pieces'], idx % h['num_pieces']].imshow(piece.cpu().squeeze())\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the predicted reconstructed image.\n",
    "figs, axs = plt.subplots(h['num_pieces'], h['num_pieces'], figsize=(4, 4), sharex=True, sharey=True)\n",
    "# remove x, y ticks\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "for idx, piece in enumerate(predicted_pieces[batch_idx]):\n",
    "    axs[idx // h['num_pieces'], idx % h['num_pieces']].imshow(piece.cpu().squeeze())\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T13:18:26.860042700Z",
     "start_time": "2024-05-16T13:18:18.160834400Z"
    }
   },
   "id": "40261faf2ede3575"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
