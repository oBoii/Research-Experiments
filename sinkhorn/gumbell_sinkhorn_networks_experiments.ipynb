{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:03:30.228468700Z",
     "start_time": "2024-05-17T12:03:24.890128700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## Torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "h = {  # hyperparameters\n",
    "    'dataset': 'MNIST',\n",
    "    'in_channels': 1,\n",
    "    'num_pieces': 8,\n",
    "    'image_size': 28,\n",
    "    'hidden_channels': 32,\n",
    "    'kernel_size': 5,\n",
    "    'tau': 0.1,\n",
    "    'n_sink_iter': 20,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 1e-4,\n",
    "    'checkpoint_path': './saved_models',\n",
    "    'dataset_path': '../data',\n",
    "    'batch_size': 64\n",
    "}\n",
    "train = True  # Set this to false if you only want to evaluate the model\n",
    "use_wandb = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:39:47.885953400Z",
     "start_time": "2024-05-17T12:39:47.868501700Z"
    }
   },
   "id": "2872261e06ec0e70"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def log_sinkhorn(log_alpha, n_iter):\n",
    "    \"\"\"Performs incomplete Sinkhorn normalization to log_alpha.\n",
    "    By a theorem by Sinkhorn and Knopp [1], a sufficiently well-behaved  matrix\n",
    "    with positive entries can be turned into a doubly-stochastic matrix\n",
    "    (i.e. its rows and columns add up to one) via the successive row and column\n",
    "    normalization.\n",
    "\n",
    "    [1] Sinkhorn, Richard and Knopp, Paul.\n",
    "    Concerning nonnegative matrices and doubly stochastic\n",
    "    matrices. Pacific Journal of Mathematics, 1967\n",
    "    Args:\n",
    "      log_alpha: 2D tensor (a matrix of shape [N, N])\n",
    "        or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n",
    "      n_iters: number of sinkhorn iterations (in practice, as little as 20\n",
    "        iterations are needed to achieve decent convergence for N~100)\n",
    "    Returns:\n",
    "      A 3D tensor of close-to-doubly-stochastic matrices (2D tensors are\n",
    "        converted to 3D tensors with batch_size equals to 1)\n",
    "    \"\"\"\n",
    "    for _ in range(n_iter):\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
    "    return log_alpha.exp()\n",
    "\n",
    "\n",
    "def matching(alpha):\n",
    "    # Negate the probability matrix to serve as cost matrix. This function\n",
    "    # yields two lists, the row and colum indices for all entries in the\n",
    "    # permutation matrix we should set to 1.\n",
    "    row, col = linear_sum_assignment(-alpha)\n",
    "\n",
    "    # Create the permutation matrix.\n",
    "    permutation_matrix = coo_matrix((np.ones_like(row), (row, col))).toarray()\n",
    "    return torch.from_numpy(permutation_matrix)\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, device='cpu', eps=1e-20):\n",
    "    \"\"\"Samples arbitrary-shaped standard gumbel variables.\n",
    "    Args:\n",
    "      shape: list of integers\n",
    "      eps: float, for numerical stability\n",
    "    Returns:\n",
    "      A sample of standard Gumbel random variables\n",
    "    \"\"\"\n",
    "    u = torch.rand(shape, device=device)\n",
    "    return -torch.log(-torch.log(u + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_sinkhorn(log_alpha, tau, n_iter):\n",
    "    \"\"\" Sample a permutation matrix from the Gumbel-Sinkhorn distribution\n",
    "    with parameters given by log_alpha and temperature tau.\n",
    "\n",
    "    Args:\n",
    "      log_alpha: Logarithm of assignment probabilities. In our case this is\n",
    "        of dimensionality [num_pieces, num_pieces].\n",
    "      tau: Temperature parameter, the lower the value for tau the more closely\n",
    "        we follow a categorical sampling.\n",
    "    \"\"\"\n",
    "    # Sample Gumbel noise.\n",
    "    gumbel_noise = sample_gumbel(log_alpha.shape, device=log_alpha.device)\n",
    "\n",
    "    # Apply the Sinkhorn operator!\n",
    "    sampled_perm_mat = log_sinkhorn((log_alpha + gumbel_noise) / tau, n_iter)\n",
    "    return sampled_perm_mat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:03:30.259961600Z",
     "start_time": "2024-05-17T12:03:30.244096700Z"
    }
   },
   "id": "caaa9de4da4da98d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_image(image: torch.Tensor, num_pieces: int):\n",
    "    \"\"\"Randomly chunk a single image.\n",
    "    Args:\n",
    "      image: Image [channels, height, width].\n",
    "\n",
    "    Returns:\n",
    "      pieces: Image chunks in their original positions. [num_pieces, channels,\n",
    "        height // num_pieces, width // num_pieces]\n",
    "      random_pieces: Image chunks in their randomly permuted positions.\n",
    "      permute_index: List of permuted indices.\n",
    "    \"\"\"\n",
    "    # Get image dimensions.\n",
    "    height, width = image.shape[-2:]\n",
    "\n",
    "    # Get piece dimensions.\n",
    "    piece_height = height // num_pieces\n",
    "    piece_width = width // num_pieces\n",
    "    pieces = []\n",
    "\n",
    "    # Obtain indices for each of the image chunks.\n",
    "    for p_h in range(num_pieces):\n",
    "        for p_w in range(num_pieces):\n",
    "            left = p_w * piece_width\n",
    "            right = left + piece_width\n",
    "            top = p_h * piece_height\n",
    "            bottom = top + piece_height\n",
    "            piece = image[:, top:bottom, left:right]\n",
    "            pieces.append(piece)\n",
    "\n",
    "    pieces = torch.stack(pieces, 0)\n",
    "\n",
    "    # Randomly permute the index of the pieces.\n",
    "    permute_index = torch.randperm(num_pieces ** 2)\n",
    "    random_pieces = pieces[permute_index]\n",
    "    return pieces, random_pieces, permute_index\n",
    "\n",
    "\n",
    "def batch_chunk_image(images: torch.Tensor, num_pieces: int):\n",
    "    \"\"\"Randomly chunk a batch of images.\n",
    "    Args:\n",
    "      image: Images [batch, channels, height, width].\n",
    "\n",
    "    Returns:\n",
    "      pieces: Batch of image chunks in their original positions. [batch,\n",
    "        num_pieces, channels, height // num_pieces, width // num_pieces]\n",
    "      random_pieces: Batch of image chunks in their randomly permuted positions.\n",
    "         [batch, num_pieces, channels, height // num_pieces, width // num_pieces]\n",
    "      permute_index: Batch of permutation lists. [batch, num_pieces**2]\n",
    "    \"\"\"\n",
    "    batch_pieces, batch_random_pieces, batch_permute_index = [], [], []\n",
    "    for image in images:\n",
    "        pieces, random_pieces, permute_index = chunk_image(image, num_pieces)\n",
    "\n",
    "        batch_pieces.append(pieces)\n",
    "        batch_random_pieces.append(random_pieces)\n",
    "        batch_permute_index.append(permute_index)\n",
    "    return torch.stack(batch_pieces, 0), torch.stack(batch_random_pieces, 0), torch.stack(batch_permute_index, 0)\n",
    "\n",
    "\n",
    "def inverse_permutation_for_image(X, permutation_matrix):\n",
    "    # temp, make `permutation_matrix` transposed\n",
    "    # permutation_matrix = permutation_matrix.transpose(1, 2)\n",
    "\n",
    "    \"\"\"Apply the inverse of a permutation (its transpose) to a batch of image\n",
    "       chunks.\n",
    "    Args:\n",
    "      X: Batched sets of image chunks. [batch, num_pieces, channels, height, width]\n",
    "      permutation_matrix: float, for numerical stability\n",
    "\n",
    "    Returns:\n",
    "      Permuted set of image chunks.\n",
    "    \"\"\"\n",
    "    return torch.einsum(\"bpq,bpchw->bqchw\", (permutation_matrix, X)).contiguous()\n",
    "\n",
    "\n",
    "# Example of `inverse_permutation_for_image`\n",
    "X = torch.rand((64, 4, 1, 14, 14))\n",
    "permutation_matrix = torch.rand((64, 4, 4))\n",
    "first1 = inverse_permutation_for_image(X, permutation_matrix)[0]  # (4, 1, 14, 14)\n",
    "first1 = first1.flatten()  # (1, 784)\n",
    "\n",
    "X_first = X[0]  # (4, 1, 14, 14)\n",
    "X_first = X_first.flatten(start_dim=1)  # (4, 196)\n",
    "\n",
    "permutation_matrix_first = permutation_matrix[0]  # (4, 4)\n",
    "permutation_matrix_first = permutation_matrix_first.T  # (4, 4)\n",
    "X_first_permuted = torch.mm(permutation_matrix_first, X_first)  # (4, 196)\n",
    "\n",
    "X_first_permuted = X_first_permuted.reshape(4, 1, 14, 14)  # (4, 1, 14, 14)\n",
    "X_first_permuted = X_first_permuted.flatten()  # (1, 784)\n",
    "\n",
    "# e.g.: P^T*X = \\tilde{X}, with row(X) = all pixels of a single piece\n",
    "torch.allclose(first1, X_first_permuted)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:03:30.319173700Z",
     "start_time": "2024-05-17T12:03:30.259961600Z"
    }
   },
   "id": "5bcb9f99ea136d5f"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([64, 4, 1, 14, 14]), torch.Size([64, 4, 4]))"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SinkhornConvNet(pl.LightningModule):\n",
    "    def __init__(self, in_channels, num_pieces, image_size, hidden_channels, kernel_size, tau=1.0, n_sink_iter=20):\n",
    "        super().__init__()\n",
    "\n",
    "        # store these for later use.\n",
    "        self.tau = tau\n",
    "        self.n_sink_iter = n_sink_iter\n",
    "        self.num_pieces = num_pieces\n",
    "\n",
    "        self.g_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_channels)\n",
    "        )\n",
    "\n",
    "        # calculate the size of a single piece in pixels\n",
    "        piece_size = image_size // num_pieces\n",
    "\n",
    "        # calculate the size of a single piece in pixels after 1 max pooling\n",
    "        piece_size_after_conv = (piece_size) // (2 * 1)\n",
    "\n",
    "        self.g_2 = nn.Linear(piece_size_after_conv ** 2 * hidden_channels, num_pieces ** 2, bias=False)\n",
    "\n",
    "    def forward(self, batch_pieces):\n",
    "        # in: (64, 4, 1, 14, 14): 4 pieces of 14x14 images\n",
    "        # out: (64, 4, 1, 14, 14), (64, 4, 4): 4 pieces of 14x14 images and permutation matrices\n",
    "        batch_size = batch_pieces.shape[0]\n",
    "\n",
    "        # Switch batch and piece dimensions. We want to apply the same network to each of the pieces.\n",
    "        pieces = batch_pieces.transpose(0, 1).contiguous()  # (4, 64, 1, 14, 14)\n",
    "\n",
    "        # Apply g_1 to each of the pieces.\n",
    "        conv_pieces = []\n",
    "        for piece in pieces:\n",
    "            piece = self.g_1(piece)\n",
    "            conv_piece = piece.reshape(batch_size, -1)\n",
    "            conv_pieces.append(conv_piece)\n",
    "\n",
    "        # Apply g_2 to each of the pieces.\n",
    "        latent_pieces = []\n",
    "        for piece in conv_pieces:\n",
    "            latent_piece = self.g_2(piece)\n",
    "            latent_pieces.append(latent_piece)\n",
    "\n",
    "        # Create a matrix of log unnormalized assignment probabilities. After this\n",
    "        # the batch dimension is batch in the first position.\n",
    "        log_alphas = torch.stack(latent_pieces, 1)  # (64, 4, 4)\n",
    "\n",
    "        # During training, we sample from the Gumbel-Sinkhorn distribution.\n",
    "        if self.training:\n",
    "            permutation_matrices = gumbel_sinkhorn(log_alphas, tau=self.tau, n_iter=self.n_sink_iter)\n",
    "\n",
    "        # During eval, we solve the linear assignment problem.\n",
    "        else:\n",
    "            permutation_matrices = torch.stack([\n",
    "                matching(log_alpha)\n",
    "                for log_alpha in log_alphas.cpu().detach().numpy()]\n",
    "            ).float().to(log_alphas.device)\n",
    "\n",
    "        # We obtain the ordered pieces as predicted by our network\n",
    "        ordered_pieces = inverse_permutation_for_image(batch_pieces, permutation_matrices)\n",
    "\n",
    "        # Return the ordered pieces, along with the predicted permutation.\n",
    "        # We will inspect the predicted permutation matrices during test time.\n",
    "        return ordered_pieces, permutation_matrices\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, _ = batch\n",
    "        pieces, random_pieces, _ = batch_chunk_image(inputs, self.num_pieces)\n",
    "        pieces, random_pieces = pieces.to(self.device), random_pieces.to(self.device)\n",
    "        ordered_pieces, _ = self(random_pieces)\n",
    "        loss = torch.nn.functional.mse_loss(ordered_pieces, pieces, reduction='sum')\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = SinkhornConvNet(\n",
    "    in_channels=1, num_pieces=2, image_size=28, hidden_channels=32, kernel_size=5, tau=0.1, n_sink_iter=20)\n",
    "\n",
    "random_pieces = torch.rand((64, 4, 1, 14, 14))\n",
    "res1, res2 = model(random_pieces)\n",
    "res1.shape, res2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:03:30.491469800Z",
     "start_time": "2024-05-17T12:03:30.319173700Z"
    }
   },
   "id": "140f1eaf4a294ae7"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# fully generated via ChatGPT\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Directory for the shifted dataset\n",
    "shifted_dir = '../data/MNIST_shift/raw'\n",
    "os.makedirs(shifted_dir, exist_ok=True)\n",
    "\n",
    "# Define the transform and load the original MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='../data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=len(trainset), drop_last=True, shuffle=False)\n",
    "test_loader = DataLoader(testset, batch_size=len(testset), drop_last=False, shuffle=False)\n",
    "\n",
    "\n",
    "# Function to shift the images\n",
    "def shift_image(image, shift_x, shift_y):\n",
    "    image_np = image.numpy()\n",
    "    shifted_image = np.zeros_like(image_np)\n",
    "    if shift_x > 0:\n",
    "        shifted_image[:, shift_x:] = image_np[:, :-shift_x]\n",
    "    elif shift_x < 0:\n",
    "        shifted_image[:, :shift_x] = image_np[:, -shift_x:]\n",
    "    else:\n",
    "        shifted_image = image_np\n",
    "    if shift_y > 0:\n",
    "        shifted_image[shift_y:, :] = shifted_image[:-shift_y, :]\n",
    "    elif shift_y < 0:\n",
    "        shifted_image[:shift_y, :] = shifted_image[-shift_y:, :]\n",
    "    return torch.tensor(shifted_image)\n",
    "\n",
    "\n",
    "def create_shifted_dataset(loader, output_images_file, output_labels_file):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for data, target in loader:\n",
    "        for i in range(data.shape[0]):\n",
    "            shift_x = random.randint(-5, 5)\n",
    "            shift_y = random.randint(-5, 5)\n",
    "            shifted_img = shift_image(data[i, 0], shift_x, shift_y)\n",
    "            images.append(shifted_img.flatten().numpy())\n",
    "            labels.append(target[i].item())\n",
    "\n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    labels = np.array(labels, dtype=np.uint8)\n",
    "\n",
    "    # Save the images\n",
    "    with open(output_images_file, 'wb') as img_file:\n",
    "        img_file.write(b'\\x00\\x00\\x08\\x03')\n",
    "        img_file.write(len(images).to_bytes(4, byteorder='big'))\n",
    "        img_file.write((28).to_bytes(4, byteorder='big'))\n",
    "        img_file.write((28).to_bytes(4, byteorder='big'))\n",
    "        img_file.write(images.tobytes())\n",
    "\n",
    "    # Save the labels\n",
    "    with open(output_labels_file, 'wb') as lbl_file:\n",
    "        lbl_file.write(b'\\x00\\x00\\x08\\x01')\n",
    "        lbl_file.write(len(labels).to_bytes(4, byteorder='big'))\n",
    "        lbl_file.write(labels.tobytes())\n",
    "\n",
    "\n",
    "# Create the shifted training and test datasets\n",
    "create_shifted_dataset(train_loader, os.path.join(shifted_dir, 'train-images-idx3-ubyte'),\n",
    "                       os.path.join(shifted_dir, 'train-labels-idx1-ubyte'))\n",
    "create_shifted_dataset(test_loader, os.path.join(shifted_dir, 't10k-images-idx3-ubyte'),\n",
    "                       os.path.join(shifted_dir, 't10k-labels-idx1-ubyte'))\n",
    "\n",
    "\n",
    "# Optionally, compress the files\n",
    "def compress_file(file_path):\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        with gzip.open(file_path + '.gz', 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "compress_file(os.path.join(shifted_dir, 'train-images-idx3-ubyte'))\n",
    "compress_file(os.path.join(shifted_dir, 'train-labels-idx1-ubyte'))\n",
    "compress_file(os.path.join(shifted_dir, 't10k-images-idx3-ubyte'))\n",
    "compress_file(os.path.join(shifted_dir, 't10k-labels-idx1-ubyte'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:03:50.582211800Z",
     "start_time": "2024-05-17T12:03:30.491469800Z"
    }
   },
   "id": "40f28b95f6a8bff4"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class MNIST_SHIFT(datasets.MNIST):\n",
    "    # MNIST will look at dir classname\n",
    "    def download(self) -> None:\n",
    "        return \n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "trainset = MNIST_SHIFT(root='../data', train=True, download=True, transform=transform)\n",
    "testset = MNIST_SHIFT(root='../data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, h['batch_size'], drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, h['batch_size'], drop_last=False, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:03:50.660891700Z",
     "start_time": "2024-05-17T12:03:50.582211800Z"
    }
   },
   "id": "8eb6d5226a5f635"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display a few images\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f984d4fbc7b6bcd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "date_identifier = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if use_wandb:\n",
    "    wandb_logger = WandbLogger()\n",
    "    wandb.init(project='sinkhorn_netw',\n",
    "               name=f'sinkhorn_mnist_{h[\"num_pieces\"]}_pieces_{date_identifier}')\n",
    "\n",
    "    wandb.config.update(h)\n",
    "\n",
    "# Initialize the model\n",
    "model = SinkhornConvNet(in_channels=h['in_channels'],\n",
    "                        num_pieces=h['num_pieces'],\n",
    "                        image_size=h['image_size'],\n",
    "                        hidden_channels=h['hidden_channels'],\n",
    "                        kernel_size=h['kernel_size'],\n",
    "                        tau=h['tau'],\n",
    "                        n_sink_iter=h['n_sink_iter'])\n",
    "\n",
    "if train:\n",
    "    # log_progress_callback; simply prints the progress to the console\n",
    "    class LogProgressCallback(pl.Callback):\n",
    "        def on_train_epoch_end(self, trainer, pl_module):\n",
    "            print(f\"Epoch {trainer.current_epoch}, \"\n",
    "                  f\"Loss {trainer.callback_metrics['train_loss']:.2f}\")\n",
    "\n",
    "\n",
    "    # Pass the callback to the Trainer\n",
    "\n",
    "    # Pass the callback to the Trainer\n",
    "    trainer = pl.Trainer(max_epochs=h['epochs'], callbacks=[LogProgressCallback()],\n",
    "                         logger=wandb_logger if use_wandb else None)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(h['checkpoint_path'], 'model.pth'))\n",
    "\n",
    "    # Finish the run if we it was running\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9614d75c960f9ff8"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa5904e9858c46e3b74e172a202a1b5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▃▃▃▃▂▂▂▂▂▂▄▂▂▂▂▂▂▂▁▂▃▃▃▂▃▂▃▂▃▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss</td><td>3.26375</td></tr><tr><td>trainer/global_step</td><td>18699</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">sinkhorn_mnist_8_pieces_20240517-140350</strong> at: <a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw/runs/wnduqzl9' target=\"_blank\">https://wandb.ai/eccv_tanmoy/sinkhorn_netw/runs/wnduqzl9</a><br/> View project at: <a href='https://wandb.ai/eccv_tanmoy/sinkhorn_netw' target=\"_blank\">https://wandb.ai/eccv_tanmoy/sinkhorn_netw</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240517_140352-wnduqzl9\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:36:09.144582900Z",
     "start_time": "2024-05-17T12:36:00.613621100Z"
    }
   },
   "id": "bc3e27be683c3678"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 64 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 64 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"{h['checkpoint_path']}/model.pth\"))\n",
    "\n",
    "image_batch, _ = next(iter(test_loader))\n",
    "pieces, random_pieces, perm_list = batch_chunk_image(image_batch, h['num_pieces'])\n",
    "# pieces, random_pieces = pieces.to(device), random_pieces.to(device)\n",
    "\n",
    "# Make sure we are evaluating!\n",
    "model.eval()\n",
    "\n",
    "# Predict the correctly ordered pieces.\n",
    "predicted_pieces, _ = model(random_pieces)\n",
    "\n",
    "# Select an image from the batch.\n",
    "batch_idx = 3\n",
    "\n",
    "# Plot the original scrambed image.\n",
    "figs, axs = plt.subplots(h['num_pieces'], h['num_pieces'], figsize=(4, 4), sharex=True, sharey=True)\n",
    "# remove x, y ticks\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "for idx, piece in enumerate(random_pieces[batch_idx]):\n",
    "    axs[idx // h['num_pieces'], idx % h['num_pieces']].imshow(piece.cpu().squeeze())\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the predicted reconstructed image.\n",
    "figs, axs = plt.subplots(h['num_pieces'], h['num_pieces'], figsize=(4, 4), sharex=True, sharey=True)\n",
    "# remove x, y ticks\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for idx, piece in enumerate(predicted_pieces[batch_idx]):\n",
    "    axs[idx // h['num_pieces'], idx % h['num_pieces']].imshow(piece.cpu().squeeze())\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:37:37.746543200Z",
     "start_time": "2024-05-17T12:37:34.197911300Z"
    }
   },
   "id": "40261faf2ede3575"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
