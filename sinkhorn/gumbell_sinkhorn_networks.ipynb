{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.403675800Z",
     "start_time": "2024-05-21T14:04:45.323664Z"
    }
   },
   "id": "8d9235e88c9fdffd"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "('saved_models/model.pth', <http.client.HTTPMessage at 0x1bce768fcd0>)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Download a pretrained model.\n",
    "# if not os.path.exists('saved_models'):\n",
    "#     os.mkdir('saved_models')\n",
    "# !wget -O 'saved_models/model.pth' https://raw.githubusercontent.com/david-knigge/uvadlc2-tutorial-Gumbel-Sinkhorn-Networks/main/model.pth\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.mkdir('saved_models')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/david-knigge/uvadlc2-tutorial-Gumbel-Sinkhorn-Networks/main/model.pth'\n",
    "urllib.request.urlretrieve(url, 'saved_models/model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.451223800Z",
     "start_time": "2024-05-21T14:04:45.343953700Z"
    }
   },
   "id": "547198692c78282d"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "\n",
    "# Path to the folder where the datasets are be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./saved_models/\"\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.451223800Z",
     "start_time": "2024-05-21T14:04:45.403675800Z"
    }
   },
   "id": "14e23fb00a550401"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def log_sinkhorn(log_alpha, n_iter):\n",
    "    \"\"\"Performs incomplete Sinkhorn normalization to log_alpha.\n",
    "    By a theorem by Sinkhorn and Knopp [1], a sufficiently well-behaved  matrix\n",
    "    with positive entries can be turned into a doubly-stochastic matrix\n",
    "    (i.e. its rows and columns add up to one) via the successive row and column\n",
    "    normalization.\n",
    "\n",
    "    [1] Sinkhorn, Richard and Knopp, Paul.\n",
    "    Concerning nonnegative matrices and doubly stochastic\n",
    "    matrices. Pacific Journal of Mathematics, 1967\n",
    "    Args:\n",
    "      log_alpha: 2D tensor (a matrix of shape [N, N])\n",
    "        or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n",
    "      n_iters: number of sinkhorn iterations (in practice, as little as 20\n",
    "        iterations are needed to achieve decent convergence for N~100)\n",
    "    Returns:\n",
    "      A 3D tensor of close-to-doubly-stochastic matrices (2D tensors are\n",
    "        converted to 3D tensors with batch_size equals to 1)\n",
    "    \"\"\"\n",
    "    for _ in range(n_iter):\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
    "    return log_alpha.exp()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.451223800Z",
     "start_time": "2024-05-21T14:04:45.419416700Z"
    }
   },
   "id": "5d9822fd9d67f51b"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.8560, 0.7285, 0.3317],\n        [0.3967, 0.2992, 0.8681],\n        [0.3998, 0.8810, 0.4031]])"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix containing random numbers.\n",
    "X = torch.rand((3, 3))\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.453786100Z",
     "start_time": "2024-05-21T14:04:45.435327800Z"
    }
   },
   "id": "25ac062be31d7def"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4759, 0.3443, 0.1798],\n        [0.2649, 0.1699, 0.5653],\n        [0.2593, 0.4858, 0.2549]])"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the Sinkhorn operator for 20 iterations.\n",
    "S_X = log_sinkhorn(torch.log(X), n_iter=20)\n",
    "S_X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.466999300Z",
     "start_time": "2024-05-21T14:04:45.453786100Z"
    }
   },
   "id": "a1380a1543b40b9c"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Check whether rows sum to 1.\n",
    "assert torch.allclose(S_X.sum(dim=0), torch.ones(S_X.shape[0]))\n",
    "\n",
    "# Check whether columns sum to 1.\n",
    "assert torch.allclose(S_X.sum(dim=1), torch.ones(S_X.shape[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.513932400Z",
     "start_time": "2024-05-21T14:04:45.466999300Z"
    }
   },
   "id": "149c78af73beeaef"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 0],\n        [0, 0, 1],\n        [1, 0, 0]])"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Scipy\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "def matching(alpha):\n",
    "    # Negate the probability matrix to serve as cost matrix. This function\n",
    "    # yields two lists, the row and colum indices for all entries in the\n",
    "    # permutation matrix we should set to 1.\n",
    "    row, col = linear_sum_assignment(-alpha)\n",
    "\n",
    "    # Create the permutation matrix.\n",
    "    permutation_matrix = coo_matrix((np.ones_like(row), (row, col))).toarray()\n",
    "    return torch.from_numpy(permutation_matrix)\n",
    "\n",
    "# Example usage:\n",
    "temp = torch.rand((3, 3))\n",
    "matching(temp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.523888300Z",
     "start_time": "2024-05-21T14:04:45.483815600Z"
    }
   },
   "id": "5a2d9555edffe7db"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 0, 0],\n        [0, 0, 1],\n        [0, 1, 0]])"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching(S_X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.523888300Z",
     "start_time": "2024-05-21T14:04:45.498544400Z"
    }
   },
   "id": "8cc89997c3ab7a71"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, device='cpu', eps=1e-20):\n",
    "    \"\"\"Samples arbitrary-shaped standard gumbel variables.\n",
    "    Args:\n",
    "      shape: list of integers\n",
    "      eps: float, for numerical stability\n",
    "    Returns:\n",
    "      A sample of standard Gumbel random variables\n",
    "    \"\"\"\n",
    "    u = torch.rand(shape, device=device)\n",
    "    return -torch.log(-torch.log(u + eps) + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.533527300Z",
     "start_time": "2024-05-21T14:04:45.513932400Z"
    }
   },
   "id": "5d1c0462ed97125b"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.4847,  2.4784,  0.5813],\n        [ 0.0427,  2.3305,  0.7799],\n        [ 1.8414,  0.1801, -0.5160]])"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gumbel((3, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.577428100Z",
     "start_time": "2024-05-21T14:04:45.533527300Z"
    }
   },
   "id": "c5bb82978013a955"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def gumbel_sinkhorn(log_alpha, tau, n_iter):\n",
    "    \"\"\" Sample a permutation matrix from the Gumbel-Sinkhorn distribution\n",
    "    with parameters given by log_alpha and temperature tau.\n",
    "\n",
    "    Args:\n",
    "      log_alpha: Logarithm of assignment probabilities. In our case this is\n",
    "        of dimensionality [num_pieces, num_pieces].\n",
    "      tau: Temperature parameter, the lower the value for tau the more closely\n",
    "        we follow a categorical sampling.\n",
    "    \"\"\"\n",
    "    # Sample Gumbel noise.\n",
    "    gumbel_noise = sample_gumbel(log_alpha.shape, device=log_alpha.device)\n",
    "\n",
    "    # Apply the Sinkhorn operator!\n",
    "    sampled_perm_mat = log_sinkhorn((log_alpha + gumbel_noise) / tau, n_iter)\n",
    "    return sampled_perm_mat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.588317400Z",
     "start_time": "2024-05-21T14:04:45.545727100Z"
    }
   },
   "id": "54db0a435a586a0"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00, 9.9998e-01, 2.5007e-02],\n        [0.0000e+00, 2.3338e-05, 9.7499e-01],\n        [1.0000e+00, 4.4447e-27, 3.6876e-34]])"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For low values of tau the sampled matrices are closer to actual\n",
    "# permutation matrices.\n",
    "gumbel_sinkhorn(X, tau=0.01, n_iter=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.588317400Z",
     "start_time": "2024-05-21T14:04:45.563893800Z"
    }
   },
   "id": "eefd3cec71eedd53"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3555, 0.3361, 0.3084],\n        [0.3041, 0.3570, 0.3390],\n        [0.3405, 0.3069, 0.3526]])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For higher values of tau the sampled matrices are 'less categorical'\n",
    "gumbel_sinkhorn(X, tau=10, n_iter=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.601212700Z",
     "start_time": "2024-05-21T14:04:45.577428100Z"
    }
   },
   "id": "530000fc82aebaae"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix containing random numbers. Let PyTorch know we want gradients.\n",
    "X = torch.rand((3, 3), requires_grad=True)\n",
    "\n",
    "# Sample a permutation matrix from the Gumbel-Sinkhorn distribution.\n",
    "P = gumbel_sinkhorn(X * 1000, tau=1, n_iter=2000)\n",
    "P.sum().backward()\n",
    "\n",
    "X.grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.723553700Z",
     "start_time": "2024-05-21T14:04:45.597272700Z"
    }
   },
   "id": "8876175eea43a66e"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SinkhornConvNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            num_pieces: int,\n",
    "            image_size: int,\n",
    "            hidden_channels: int,\n",
    "            kernel_size: int,\n",
    "            tau: float = 1.0,\n",
    "            n_sink_iter: int = 20,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # store these for later use.\n",
    "        self.tau = tau\n",
    "        self.n_sink_iter = n_sink_iter\n",
    "\n",
    "        self.g_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_channels)\n",
    "        )\n",
    "\n",
    "        # calculate the size of a single piece in pixels\n",
    "        piece_size = image_size // num_pieces\n",
    "\n",
    "        # calculate the size of a single piece in pixels after 1 max pooling\n",
    "        piece_size_after_conv = (piece_size) // (2 * 1)\n",
    "\n",
    "        self.g_2 = nn.Linear(piece_size_after_conv ** 2 * hidden_channels, num_pieces ** 2, bias=False)\n",
    "\n",
    "    def forward(self, batch_pieces):\n",
    "        # in: (torch.Size([64, 4, 1, 14, 14]))\n",
    "        # out: (torch.Size([64, 4, 1, 14, 14]), torch.Size([64, 4, 4]))\n",
    "        batch_size = batch_pieces.shape[0]\n",
    "\n",
    "        # Switch batch and piece dimensions. We want to apply the same network\n",
    "        # to each of the pieces.\n",
    "        pieces = batch_pieces.transpose(0, 1).contiguous()\n",
    "\n",
    "        # Apply g_1 to each of the pieces.\n",
    "        conv_pieces = []\n",
    "        for piece in pieces:\n",
    "            piece = self.g_1(piece)\n",
    "            conv_piece = piece.reshape(batch_size, -1)\n",
    "            conv_pieces.append(conv_piece)\n",
    "\n",
    "        # Apply g_2 to each of the pieces.\n",
    "        latent_pieces = []\n",
    "        for piece in conv_pieces:\n",
    "            latent_piece = self.g_2(piece)\n",
    "            latent_pieces.append(latent_piece)\n",
    "        \n",
    "\n",
    "        # Create a matrix of log unnormalized assignment probabilities. After this\n",
    "        # the batch dimension is batch in the first position.\n",
    "        log_alphas = torch.stack(latent_pieces, 1)\n",
    "\n",
    "        # During training, we sample from the Gumbel-Sinkhorn distribution.\n",
    "        if self.training:\n",
    "            permutation_matrices = gumbel_sinkhorn(log_alphas, tau=self.tau, n_iter=self.n_sink_iter)\n",
    "\n",
    "        # During eval, we solve the linear assignment problem.\n",
    "        else:\n",
    "            permutation_matrices = torch.stack([\n",
    "                matching(log_alpha)\n",
    "                for log_alpha in log_alphas.cpu().detach().numpy()]\n",
    "            ).float().to(log_alphas.device)\n",
    "\n",
    "        # We obtain the ordered pieces as predicted by our network\n",
    "        ordered_pieces = inverse_permutation_for_image(batch_pieces, permutation_matrices)\n",
    "\n",
    "        # Return the ordered pieces, along with the predicted permutation.\n",
    "        # We will inspect the predicted permutation matrices during test time.\n",
    "        return ordered_pieces, permutation_matrices\n",
    "    \n",
    "# # test\n",
    "# model = SinkhornConvNet(\n",
    "#     in_channels=1,\n",
    "#     num_pieces=2,\n",
    "#     image_size=28,\n",
    "#     hidden_channels=32,\n",
    "#     kernel_size=5,\n",
    "#     tau=0.1,\n",
    "#     n_sink_iter=20\n",
    "# ).to(device)\n",
    "# \n",
    "# random_pieces = torch.rand((64, 4, 1, 14, 14)).to(device)\n",
    "# res1, res2 = model(random_pieces)\n",
    "# res1.shape, res2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.759986200Z",
     "start_time": "2024-05-21T14:04:45.723553700Z"
    }
   },
   "id": "3b03ba8787c95b89"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def chunk_image(image: torch.Tensor, num_pieces: int):\n",
    "    \"\"\"Randomly chunk a single image.\n",
    "    Args:\n",
    "      image: Image [channels, height, width].\n",
    "\n",
    "    Returns:\n",
    "      pieces: Image chunks in their original positions. [num_pieces, channels,\n",
    "        height // num_pieces, width // num_pieces]\n",
    "      random_pieces: Image chunks in their randomly permuted positions.\n",
    "      permute_index: List of permuted indices.\n",
    "    \"\"\"\n",
    "    # Get image dimensions.\n",
    "    height, width = image.shape[-2:]\n",
    "\n",
    "    # Get piece dimensions.\n",
    "    piece_height = height // num_pieces\n",
    "    piece_width = width // num_pieces\n",
    "    pieces = []\n",
    "\n",
    "    # Obtain indices for each of the image chunks.\n",
    "    for p_h in range(num_pieces):\n",
    "        for p_w in range(num_pieces):\n",
    "            left = p_w * piece_width\n",
    "            right = left + piece_width\n",
    "            top = p_h * piece_height\n",
    "            bottom = top + piece_height\n",
    "            piece = image[:, top:bottom, left:right]\n",
    "            pieces.append(piece)\n",
    "\n",
    "    pieces = torch.stack(pieces, 0)\n",
    "\n",
    "    # Randomly permute the index of the pieces.\n",
    "    permute_index = torch.randperm(num_pieces ** 2)\n",
    "    random_pieces = pieces[permute_index]\n",
    "    return pieces, random_pieces, permute_index\n",
    "\n",
    "\n",
    "def batch_chunk_image(images: torch.Tensor, num_pieces: int):\n",
    "    \"\"\"Randomly chunk a batch of images.\n",
    "    Args:\n",
    "      image: Images [batch, channels, height, width].\n",
    "\n",
    "    Returns:\n",
    "      pieces: Batch of image chunks in their original positions. [batch,\n",
    "        num_pieces, channels, height // num_pieces, width // num_pieces]\n",
    "      random_pieces: Batch of image chunks in their randomly permuted positions.\n",
    "         [batch, num_pieces, channels, height // num_pieces, width // num_pieces]\n",
    "      permute_index: Batch of permutation lists. [batch, num_pieces**2]\n",
    "    \"\"\"\n",
    "    batch_pieces, batch_random_pieces, batch_permute_index = [], [], []\n",
    "    for image in images:\n",
    "        pieces, random_pieces, permute_index = chunk_image(image, num_pieces)\n",
    "\n",
    "        batch_pieces.append(pieces)\n",
    "        batch_random_pieces.append(random_pieces)\n",
    "        batch_permute_index.append(permute_index)\n",
    "    return torch.stack(batch_pieces, 0), torch.stack(batch_random_pieces, 0), torch.stack(batch_permute_index, 0)\n",
    "\n",
    "\n",
    "def inverse_permutation_for_image(X, permutation_matrix):\n",
    "    \"\"\"Apply the inverse of a permutation (its transpose) to a batch of image\n",
    "       chunks.\n",
    "    Args:\n",
    "      X: Batched sets of image chunks. [batch, num_pieces, channels, height, width]\n",
    "      permutation_matrix: float, for numerical stability\n",
    "\n",
    "    Returns:\n",
    "      Permuted set of image chunks.\n",
    "    \"\"\"\n",
    "    return torch.einsum(\"bpq,bpchw->bqchw\", (permutation_matrix, X)).contiguous()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.763490900Z",
     "start_time": "2024-05-21T14:04:45.743962600Z"
    }
   },
   "id": "a76465e4296c07ac"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='../data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, 64, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, 64, drop_last=False, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.823517100Z",
     "start_time": "2024-05-21T14:04:45.763490900Z"
    }
   },
   "id": "1a69eb5428d6c9b0"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "image_batch, _ = next(iter(test_loader))\n",
    "\n",
    "# Chunk the images into 2 pieces\n",
    "num_pieces = 2\n",
    "\n",
    "# Select an image from the batch\n",
    "batch_idx = 0\n",
    "\n",
    "pieces, random_pieces, perm_list = batch_chunk_image(image_batch, num_pieces=num_pieces)\n",
    "pieces, random_pieces = pieces.to(device), random_pieces.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.841777900Z",
     "start_time": "2024-05-21T14:04:45.823517100Z"
    }
   },
   "id": "8d1a869f05869dde"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# res1, res2 = model(random_pieces)\n",
    "# res1.shape, res2.shape\n",
    "\n",
    "# print(image_batch.shape)\n",
    "# print(pieces.shape)\n",
    "# print(random_pieces.shape)\n",
    "# \n",
    "# model.train()\n",
    "# res1, res2 = model(random_pieces)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:45.863874300Z",
     "start_time": "2024-05-21T14:04:45.841777900Z"
    }
   },
   "id": "7a5197c146d477b1"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 4 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAFfCAYAAAB5prZOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAY90lEQVR4nO3dcXCU953f8c9KQivAIJmKW0m2sMXFmDM9RIstmZy4QqxWlVsqyFwOc9yFUo9JJiGtT3FpNbUBeXwjB1KOgSpm7nqu6ru5AHc3ltOpo86hOCbEQq4B183Z2BIVRgRWgGO0ks4IaffpHwpyZAnpJ/lZnq+W92vmmdl99qvn+a72pw8/nn2e3ZDneZ4AAIFLC7oBAMAQAhkAjCCQAcAIAhkAjCCQAcAIAhkAjCCQAcCIjKAb+KxEIqELFy5ozpw5CoVCQbeDFOV5nrq7uyVJ2dnZjDUkjed56unpUUFBgdLSxp8DmwvkCxcuqLCwMOg2AMBXnZ2duvvuu8etMRfIc+bMkSRVH3lE4dkTt/cXPylz3vbvrzzmXPuH8047184IpTvX/vbb65xrf+0bV51r41euONdCGtSAjulVSVKZHlWGZgTcUepKlC11rq353p871z6cFfwR18vxvglrensT+scPXR7OtvEkLZDr6+u1e/duRaNRFRcXa//+/SopKZnw52781zE8O0NZd0z8R5I2M8u5J5ft3TB3jvuLPSPkXps+K+xcm5GW6VwbChEok/IrHxiQoRnK4PeXNIkM97/R2ZP4u5trIJCvxd17cDkslpRndOjQIVVXV2vHjh06efKkiouLVVFRoUuXLiVjdwCQEpISyHv27NETTzyhzZs364EHHtCBAwc0a9Ysvfjii8nYHQCkBN8D+fr16zpx4oTKy8s/3UlamsrLy9XS0jKqvr+/X7FYbMQCJEPCi2vQGxhaNBB0O8Aovh9DvnLliuLxuCKRyIj1kUhEp0+PfqOsrq5OtbW1frcBjNKh0+rQe0G3AdxU4EfFa2pq1N3dPbx0dnYG3RJSVJEWa5WqtEpVKtOjQbcDjOL7DDk3N1fp6enq6uoasb6rq0t5eXmj6sPhsMJh9zMPgKlKC6UrTb88RZGvZYBBvs+QMzMztXz5cjU3Nw+vSyQSam5u1ooVK/zeHQCkjKSch1xdXa1NmzbpwQcfVElJifbu3au+vj5t3rw5GbsDgJSQlEBev369Ll++rO3btysajWrZsmVqamoa9UbfeFpWZDmdrH+fWp23+RO5n6Be0u5e+89mub9jf2L5Yefahyu+7lyb/ReXnWuBWynt9VPOtTu/+m+ca68Uz5pKO77K+kViwpr4wDVJTzttL2lX6m3dulVbt25N1uYBIOUEfpYFAGAIgQwARhDIAGAEgQwARhDIAGAEgQwARhDIAGAEgQwARhDIAGCEuS85teIP/+wJ59q/+9b3ktgJcPtIO/a2c+2vuX9ncaAGPfePVmCGDABGEMgAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYASXTt9Ezj+JBt0CgNsMM2QAMIJABgAjCGQAMIJABgAjCGQAMIJABgAjCGQAMIJABgAjCGQAMIJABgAjuHT6Jr7/wEuTqL4jaX0AU9W+92Hn2te+/F3n2kOx4kn1ceQfzplU/e2MGTIAGEEgA4ARBDIAGEEgA4ARBDIAGEEgA4ARBDIAGEEgA4ARBDIAGEEgA4AR0/7S6cEvLXeu/dqBv3auvSt91lTamdCio191rv3CD951ro1PpRlMOx89scK59v/8zh87196R5n75/xdntTnXStKPlv6ec23indOT2naqYYYMAEb4Hsg7d+5UKBQasSxevNjv3QBAyknKIYslS5boyJEjn+4kY9ofGQGApEtKUmZkZCgvLy8ZmwaAlJWUY8htbW0qKCjQwoULtXHjRp07d+6mtf39/YrFYiMWIBkSXlyD3sDQooGg2wFG8X2GXFpaqoaGBt1///26ePGiamtrtXLlSv3sZz/TnDmjP6i6rq5OtbW1frcBjNKh0+rQe0G3AdyU74FcWVk5fHvp0qUqLS3VPffco8OHD+vxxx8fVV9TU6Pq6urh+7FYTIWFhX63BahIi3WPFkmSBjWgY3o14I6AkZL+bltOTo4WLVqk9vb2MR8Ph8MKh8PJbgNQWihdaUofuuMF2wswlqSfh9zb26szZ84oPz8/2bsCgGnN90B+6qmn9Prrr+vs2bN64403tG7dOqWnp2vDhg1+7woAUorvhyzOnz+vDRs26KOPPtL8+fNVVlam48ePa/78+ZPb0PIHpIysCcv6t33svMnfvaN7Eg24/1t1ov+6c23RHvf/K8c54wSfEWnqdK49si3XuXbt7F7n2t/Kmtw8rurgT5xrf/A7v+VcG3/3g0n1MR34HsgHDx70e5MAcFvgsywAwAgCGQCMIJABwAgCGQCMIJABwAgCGQCMIJABwAgCGQCMIJABwAiz36307178K82ekz5h3SMzg/++5d899jXn2i+8eSqJnSDVDXaed67d9ld/4Fy79l+/MJV2nHw95+fOtTP+5qhz7fOvrHOuXdjY51z78eLZzrW5mz6csMbr65f+hdv2mCEDgBEEMgAYQSADgBEEMgAYQSADgBEEMgAYQSADgBEEMgAYQSADgBEEMgAYYfbS6VUzE5o7M7j9L9n/Defa+//ktHNt8Bd643bxhf9+2bn217O+7lz78pf3TqqPpZkTf3v8DY9nR91rvzqJy72/6l7qt1hPQnc61jJDBgAjCGQAMIJABgAjCGQAMIJABgAjCGQAMIJABgAjCGQAMIJABgAjCGQAMMLspdOuVv9dlXPthTcLnGsX/vFJ59r4tWvOtcCtEn+/3bn2C9Xutf/xO/9qUn2c+be/7lz7j1a971x7sOhHk+pjOmCGDABGEMgAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYITZS6fXLfpNZYRmTFiXqQ+dt3nvJGoTzpXA7SXedWlS9ff+J/f6jyex3Qotm1QfQRn0BiT9P6daZsgAYMSkA/no0aNas2aNCgoKFAqF1NjYOOJxz/O0fft25efna+bMmSovL1dbW5tf/QJAypp0IPf19am4uFj19fVjPr5r1y7t27dPBw4cUGtrq2bPnq2Kigpd4xPRAGBckz6GXFlZqcrKyjEf8zxPe/fu1dNPP62qqqGPxXzppZcUiUTU2Nioxx577PN1CwApzNdjyB0dHYpGoyovLx9el52drdLSUrW0tIz5M/39/YrFYiMWIBkSXlyD3sDQooGg2wFG8fUsi2g0KkmKRCIj1kcikeHHPquurk61tbV+tgGMqUOn1aH3gm4DuKnAz7KoqalRd3f38NLZ2Rl0S0hRRVqsVarSKlWpTI8G3Q4wiq8z5Ly8PElSV1eX8vPzh9d3dXVp2bJlY/5MOBxWOBz2sw1gTGmhdKUpfeiOF2wvwFh8nSEXFRUpLy9Pzc3Nw+tisZhaW1u1YsUKP3cFACln0jPk3t5etbd/+oWIHR0devvttzVv3jwtWLBATz75pJ577jndd999Kioq0jPPPKOCggKtXbvWz74BIOVMOpDfeustrV69evh+dXW1JGnTpk1qaGjQtm3b1NfXpy1btujq1asqKytTU1OTsrKy/OsaAFJQyPM8U0fTYrGYsrOztUpVTp9lAUzFoDegH+sVSWKsIalujLXu7m7NnTt33NrAz7IAAAwhkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHACAIZAIwgkAHAiIygG/iseDwuSfpEfZrhZQbcDVLVgK4P32asIZlujLUb2TYec4Hc3t4uSWrVkYA7we2CsYZbob29XQ899NC4NSHP87xb1I+Tjz/+WPPmzVOZHlWGZgTdDlLUoAZ0TK9KEmMNSXVjrP3iF7/QnXfeOW5t0mbI9fX12r17t6LRqIqLi7V//36VlJRM+HPp6em/bGyGMkL8kSBJfmUawlhDUv1yrN3ItvEk5U29Q4cOqbq6Wjt27NDJkydVXFysiooKXbp0KRm7A4CUkJRA3rNnj5544glt3rxZDzzwgA4cOKBZs2bpxRdfTMbuACAl+B7I169f14kTJ1ReXv7pTtLSVF5erpaWllH1/f39isViIxYgGRJeXIPewNCigaDbAUbx/RjylStXFI/HFYlERqyPRCI6ffr0qPq6ujrV1tb63QYwSodOq0PvBd0GcFOBXxhSU1Oj7u7u4aWzszPolpCiirRYq1SlVapSmR4Nuh1gFN9nyLm5uUpPT1dXV9eI9V1dXcrLyxtVHw6HFQ6H/W4DGCUtlK40/fKdblMnewJDfJ8hZ2Zmavny5Wpubh5el0gk1NzcrBUrVvi9OwBIGUk5D7m6ulqbNm3Sgw8+qJKSEu3du1d9fX3avHlzMnYHACkhKYG8fv16Xb58Wdu3b1c0GtWyZcvU1NQ06o0+AMCnknal3tatW7V169ZkbR4AUk7gZ1kAAIYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEYQyABgBIEMAEZkBN0AUse5HV90rn3va99LYicTi/UkdOeiodsvf/B/NXfOxHOTuJdISi/3/+U3nWsXffeMc22869JU2kGAmCEDgBEEMgAYQSADgBEEMgAYQSADgBEEMgAYQSADgBEEMgAYQSADgBEEMgAYwaXT8M2899wvLT4/2Otcm58+ayrtjOtXL4Me8OIa8Dzf9+Hqvd/7L861j5etdq5t2/+wc+3cvzzuXIvkYYYMAEb4Hsg7d+5UKBQasSxevNjv3QBAyknKIYslS5boyJEjn+4kgyMjADCRpCRlRkaG8vLykrFpAEhZSTmG3NbWpoKCAi1cuFAbN27UuXPnblrb39+vWCw2YgGSob/fU6wnoVhPQj29yflsY+Dz8D2QS0tL1dDQoKamJr3wwgvq6OjQypUr1dPTM2Z9XV2dsrOzh5fCwkK/WwIkSd/Z/7Fy7z+r3PvPqmj5zScJQFB8D+TKykp95Stf0dKlS1VRUaFXX31VV69e1eHDh8esr6mpUXd39/DS2dnpd0uAJOk/fOtOXXn/Xl15/151nFgQdDvAKEl/ty0nJ0eLFi1Se3v7mI+Hw2GFw+FktwEoHA4pHA4F3QZwU0k/D7m3t1dnzpxRfn5+sncFANOa74H81FNP6fXXX9fZs2f1xhtvaN26dUpPT9eGDRv83hUApBTfD1mcP39eGzZs0EcffaT58+errKxMx48f1/z58/3eFYy547D75bdVOf/euXbl1/63c+1/znvTufaG56/8psLXZkxY13KlyHmbXy445Vy7Jfusc+2fLXjNufbi86861/7+31c7185snPzvGG58D+SDBw/6vUkAuC3wWRYAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYATfrYRA5P5Ji3Nt2w8izrVfeujrTnWDA9ckbZckvbNhgTLSJv7EwYwP3T9D+X/+hvs3Pn/3yX/uXPvBvzzgXJufPtO5dua3LjjXqtG9FJPDDBkAjCCQAcAIAhkAjCCQAcAIAhkAjCCQAcAIAhkAjCCQAcAIAhkAjCCQAcAILp2GeYPRLufarP/hVjvoDXx6+9zPpdDE3zo9GfH32pxrZ539oq/7norKvJ851/5QOclr5DbHDBkAjCCQAcAIAhkAjCCQAcAIAhkAjCCQAcAIAhkAjCCQAcAIAhkAjCCQAcAILp0GkiBt1izn2t949IMkdoLphBkyABhBIAOAEQQyABhBIAOAEQQyABhBIAOAEQQyABhBIAOAEQQyABhBIAOAEVw6jXH1fqXUufbiyiQ24rPEJ9ekba9Iks585yGlzczydftffOi0c+1/u+d/+brvG/6mN9e59r/++aPOtXfpjam0AwfMkAHAiEkH8tGjR7VmzRoVFBQoFAqpsbFxxOOe52n79u3Kz8/XzJkzVV5erra2Nr/6BYCUNelA7uvrU3Fxserr68d8fNeuXdq3b58OHDig1tZWzZ49WxUVFbp27drnbhYAUtmkjyFXVlaqsrJyzMc8z9PevXv19NNPq6qqSpL00ksvKRKJqLGxUY899tjn6xYAUpivx5A7OjoUjUZVXl4+vC47O1ulpaVqaWkZ82f6+/sVi8VGLEAyeAODSnxybWjhf2wwyNezLKLRqCQpEomMWB+JRIYf+6y6ujrV1tb62QYwpu6//ZG6f/i3QbcB3FTgp73V1NSourp6+H4sFlNhYWGAHSFVZf/TL2nu6t+WJCWuXdPPt/9RwB0BI/kayHl5eZKkrq4u5efnD6/v6urSsmXLxvyZcDiscDjsZxvAmEIzMhSaEfgcBLgpX48hFxUVKS8vT83NzcPrYrGYWltbtWLFCj93BQApZ9LThd7eXrW3tw/f7+jo0Ntvv6158+ZpwYIFevLJJ/Xcc8/pvvvuU1FRkZ555hkVFBRo7dq1fvYNACln0oH81ltvafXq1cP3bxz/3bRpkxoaGrRt2zb19fVpy5Ytunr1qsrKytTU1KSsLH8vTcXUdW982Ln2R9/Z51wbDk2fwwGxnoRytw3dPrH2TzV3zvS4aPWl2F3OtX+9fpVz7V3vcDm0BZP+C1q1apU8z7vp46FQSM8++6yeffbZz9UYANxupse0AABuAwQyABhBIAOAEQQyABhBIAOAEQQyABhBIAOAEQQyABhBIAOAEdPnWleMK/0fzHOuvfcbHzjXTqfLoW8Hf9pR5lyb/Y77N1/DBmbIAGAEgQwARhDIAGAEgQwARhDIAGAEgQwARhDIAGAEgQwARhDIAGAEgQwARnBdbKq4M9u59A8iP0xiI0imJfOizrUX5893ro1fvjyVduAzZsgAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYASBDABGEMgAYIS5K/U8z5MkDWpA8gJuZhrx4v3OtX/fE3eujQ0mptKOeT29iTFvW3e997pz7WAi3bk27g1MpR04GNTQ7/ZGto0n5LlU3ULnz59XYWFh0G0AgK86Ozt19913j1tjLpATiYQuXLigOXPmKBQKDa+PxWIqLCxUZ2en5s6dG2CH/uO53Xqe56m7u1uSlJ2dzVhLAVafm+d56unpUUFBgdLSxj9KbO6QRVpa2rj/isydO9fUL9tPPLdbKzt7/A9kstizX3hut9ZEY+0G3tQDACMIZAAwYtoEcjgc1o4dOxQOh4NuxXc8N1umY8+ueG62mXtTDwBuV9NmhgwAqY5ABgAjCGQAMIJABgAjCGQAMGJaBHJ9fb3uvfdeZWVlqbS0VG+++WbQLfli586dCoVCI5bFixcH3daUHD16VGvWrFFBQYFCoZAaGxtHPO55nrZv3678/HzNnDlT5eXlamtrC6bZcTDW7EuVsTYW84F86NAhVVdXa8eOHTp58qSKi4tVUVGhS5cuBd2aL5YsWaKLFy8OL8eOHQu6pSnp6+tTcXGx6uvrx3x8165d2rdvnw4cOKDW1lbNnj1bFRUVunbt2i3u9OYYa9NDKoy1m/KMKykp8b75zW8O34/H415BQYFXV1cXYFf+2LFjh1dcXBx0G76T5L388svD9xOJhJeXl+ft3r17eN3Vq1e9cDjsff/73w+gw7Ex1qaf6TrWbsb0DPn69es6ceKEysvLh9elpaWpvLxcLS0tAXbmn7a2NhUUFGjhwoXauHGjzp07F3RLvuvo6FA0Gh3xOmZnZ6u0tNTM68hYSw3TYayNx3QgX7lyRfF4XJFIZMT6SCSiaDQaUFf+KS0tVUNDg5qamvTCCy+oo6NDK1euVE9PT9Ct+erGa2X5dWSspYbpMNbGY+7jN28nlZWVw7eXLl2q0tJS3XPPPTp8+LAef/zxADtDqmGsTQ+mZ8i5ublKT09XV1fXiPVdXV3Ky8sLqKvkycnJ0aJFi9Te3h50K7668VpZfh0Za6lhOoy18ZgO5MzMTC1fvlzNzc3D6xKJhJqbm7VixYoAO0uO3t5enTlzRvn5+UG34quioiLl5eWNeB1jsZhaW1vNvI6MtdQwHcbauIJ+V3EiBw8e9MLhsNfQ0OC9++673pYtW7ycnBwvGo0G3drn9u1vf9v78Y9/7HV0dHg//elPvfLyci83N9e7dOlS0K1NWk9Pj3fq1Cnv1KlTniRvz5493qlTp7wPP/zQ8zzPe/75572cnBzvlVde8d555x2vqqrKKyoq8j755JOAO/8UY216SIWxdjPmA9nzPG///v3eggULvMzMTK+kpMQ7fvx40C35Yv369V5+fr6XmZnp3XXXXd769eu99vb2oNuaktdee83T0PeEj1g2bdrked7Q6UjPPPOMF4lEvHA47D3yyCPe+++/H2zTY2Cs2ZcqY20sfB4yABhh+hgyANxOCGQAMIJABgAjCGQAMIJABgAjCGQAMIJABgAjCGQAMIJABgAjCGQAMIJABgAj/j9QXjAc6bTc7AAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the original scrambed image.\n",
    "figs, axs = plt.subplots(num_pieces, num_pieces, figsize=(4, 4), sharex=True, sharey=True)\n",
    "for idx, piece in enumerate(random_pieces[batch_idx]):\n",
    "    axs[idx // num_pieces, idx % num_pieces].imshow(piece.cpu().squeeze())\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:46.024055900Z",
     "start_time": "2024-05-21T14:04:45.855368700Z"
    }
   },
   "id": "cc722c5a8267fbce"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "tau = 0.1  # Temperature parameter.\n",
    "n_sink_iter = 20  # Number of iterations of Sinkhorn operator.\n",
    "\n",
    "num_pieces = 2  # Number of pieces each side.\n",
    "image_size = 28  # Original image size.\n",
    "\n",
    "in_channels = 1\n",
    "hidden_channels = 32\n",
    "kernel_size = 5\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 1e-4\n",
    "train = True  # Set this to false if you only want to evaluate the model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:46.041974800Z",
     "start_time": "2024-05-21T14:04:46.024055900Z"
    }
   },
   "id": "43a0e9c61814af47"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "model = SinkhornConvNet(\n",
    "    in_channels=in_channels,\n",
    "    num_pieces=num_pieces,\n",
    "    image_size=image_size,\n",
    "    hidden_channels=hidden_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    tau=tau,\n",
    "    n_sink_iter=n_sink_iter\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate, eps=1e-8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:04:46.063550200Z",
     "start_time": "2024-05-21T14:04:46.041974800Z"
    }
   },
   "id": "b6fcf47cd97467c3"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0|  mean loss 58.1284\n",
      "epoch 1|  mean loss 10.5026\n",
      "epoch 2|  mean loss 6.8696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[111], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      4\u001B[0m     sum_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      6\u001B[0m         inputs, _ \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;66;03m# Chunk the images into pieces\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    142\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 145\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    148\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:175\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    173\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[1;32m--> 175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_float_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, _ = data\n",
    "\n",
    "            # Chunk the images into pieces\n",
    "            pieces, random_pieces, _ = batch_chunk_image(inputs, num_pieces)\n",
    "            pieces, random_pieces = pieces.to(device), random_pieces.to(device)\n",
    "\n",
    "            # Predict the ordering of the pieces using the Sinkhorn Network\n",
    "            # in: (batch, num_pieces**2, channels, height, width)\n",
    "            # out: (batch, num_pieces**2, channels, height, width)\n",
    "            ordered_pieces, _ = model(random_pieces)\n",
    "\n",
    "            # Apply MSE Loss\n",
    "            loss = torch.nn.functional.mse_loss(ordered_pieces, pieces, reduction='sum')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "        print(f\"epoch {epoch}|  mean loss {round(sum_loss / len(train_loader.dataset), 4)}\")\n",
    "\n",
    "    if not os.path.exists('./saved_models/'):\n",
    "        os.mkdir('./saved_models')\n",
    "    torch.save(model.state_dict(), \"./saved_models/model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:05:56.373944400Z",
     "start_time": "2024-05-21T14:04:46.056044300Z"
    }
   },
   "id": "bfc1d64d116c9bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scipy\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "model = SinkhornConvNet(\n",
    "    in_channels=in_channels,\n",
    "    num_pieces=num_pieces,\n",
    "    image_size=image_size,\n",
    "    hidden_channels=hidden_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    tau=tau,\n",
    "    n_sink_iter=n_sink_iter\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(\"./saved_models/model.pth\")))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    kendall_taus = []\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, _ = data\n",
    "        pieces, random_pieces, perm_list = batch_chunk_image(inputs, num_pieces)\n",
    "        pieces, random_pieces = pieces.to(device), random_pieces.to(device)\n",
    "\n",
    "        ordered_pieces, predicted_permutation_matrices = model(random_pieces)\n",
    "\n",
    "        # Create the list of inverse permutation indices from the predicted\n",
    "        # permutation matrix.\n",
    "        predicted_perm_list = predicted_permutation_matrices.transpose(1, 2).max(1)[1]\n",
    "\n",
    "        # Obtain the Kendall-Tau correlation coefficient for the target\n",
    "        # and predicted list of permutation matrices.\n",
    "        for p1, p2 in zip(perm_list, predicted_perm_list):\n",
    "            kendall_taus.append(\n",
    "                kendalltau(p1.cpu(), p2.cpu())[0]\n",
    "            )\n",
    "\n",
    "    print(f\"Mean Kendall-Tau: {np.mean(kendall_taus)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:05:56.378434Z",
     "start_time": "2024-05-21T14:05:56.378434Z"
    }
   },
   "id": "a78508fafdb23ba3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_batch, _ = next(iter(test_loader))\n",
    "pieces, random_pieces, perm_list = batch_chunk_image(image_batch, num_pieces)\n",
    "pieces, random_pieces = pieces.to(device), random_pieces.to(device)\n",
    "\n",
    "# Make sure we are evaluating!\n",
    "model.eval()\n",
    "\n",
    "# Predict the correctly ordered pieces.\n",
    "predicted_pieces, _ = model(random_pieces)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:05:56.378434Z",
     "start_time": "2024-05-21T14:05:56.378434Z"
    }
   },
   "id": "5817a35f02bf5b6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select an image from the batch.\n",
    "batch_idx = 3\n",
    "\n",
    "# Plot the original scrambed image.\n",
    "figs, axs = plt.subplots(num_pieces, num_pieces, figsize=(4, 4), sharex=True, sharey=True)\n",
    "for idx, piece in enumerate(random_pieces[batch_idx]):\n",
    "    axs[idx // num_pieces, idx % num_pieces].imshow(piece.cpu().squeeze())\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the predicted reconstructed image.\n",
    "figs, axs = plt.subplots(num_pieces, num_pieces, figsize=(4, 4))\n",
    "for idx, piece in enumerate(predicted_pieces[batch_idx]):\n",
    "    axs[idx // num_pieces, idx % num_pieces].imshow(piece.cpu().squeeze())\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-21T14:05:56.378434Z"
    }
   },
   "id": "8cbfb58ef9c4afd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:05:56.378434Z",
     "start_time": "2024-05-21T14:05:56.378434Z"
    }
   },
   "id": "330850970afa8274"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
